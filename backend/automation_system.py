import schedule
import time
import threading
from datetime import datetime, timedelta
import sqlite3
import requests
from bs4 import BeautifulSoup
import feedparser
import logging
from typing import Dict, List
import json

from ai_engine import ai_engine

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RevolutionaryAutomationSystem:
    def __init__(self):
        self.is_running = False
        self.sources = [
            {
                'name': 'ÙˆÙƒØ§Ù„Ø© Ø³Ø§Ù†Ø§',
                'url': 'https://sana.sy/rss.xml',
                'country': 'Ø³ÙˆØ±ÙŠØ§',
                'language': 'ar',
                'active': True
            },
            {
                'name': 'Ø§Ù„ÙˆØ·Ù† Ø£ÙˆÙ†Ù„Ø§ÙŠÙ†',
                'url': 'https://alwatan.sy/rss.xml',
                'country': 'Ø³ÙˆØ±ÙŠØ§', 
                'language': 'ar',
                'active': True
            },
            {
                'name': 'Ø§Ù„Ø¬Ø²ÙŠØ±Ø©',
                'url': 'https://www.aljazeera.net/rss/all.xml',
                'country': 'Ù‚Ø·Ø±',
                'language': 'ar',
                'active': True
            },
            {
                'name': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©',
                'url': 'https://www.alarabiya.net/rss.xml',
                'country': 'Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©',
                'language': 'ar',
                'active': True
            }
        ]
        
        self.processing_stats = {
            'total_scraped': 0,
            'total_processed': 0,
            'total_published': 0,
            'last_run': None,
            'processing_time': 0,
            'errors': 0
        }
        
        self.scheduler = schedule.Scheduler()
        self._setup_schedules()
    
    def _setup_schedules(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        # ØªØ´ØºÙŠÙ„ ÙƒÙ„ 5 Ø¯Ù‚Ø§Ø¦Ù‚
        self.scheduler.every(5).minutes.do(self.run_automation_cycle)
        
        # ØªØ´ØºÙŠÙ„ ÙƒÙ„ Ø³Ø§Ø¹Ø©
        self.scheduler.every().hour.do(self.run_analytics_update)
        
        # ØªØ´ØºÙŠÙ„ ÙƒÙ„ ÙŠÙˆÙ… ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø© 6 ØµØ¨Ø§Ø­Ø§Ù‹
        self.scheduler.every().day.at("06:00").do(self.run_daily_maintenance)
        
        logger.info("ðŸ“… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø£ØªÙ…ØªØ©")
    
    def start_automation(self):
        """Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØªÙ…ØªØ©"""
        if self.is_running:
            logger.warning("Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØªÙ…ØªØ© ÙŠØ¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„")
            return
        
        self.is_running = True
        logger.info("ðŸš€ Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØªÙ…ØªØ© Ø§Ù„Ø«ÙˆØ±ÙŠ")
        
        # ØªØ´ØºÙŠÙ„ ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„
        automation_thread = threading.Thread(target=self._run_scheduler)
        automation_thread.daemon = True
        automation_thread.start()
        
        # ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© Ø£ÙˆÙ„ÙŠØ©
        self.run_automation_cycle()
    
    def stop_automation(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØªÙ…ØªØ©"""
        self.is_running = False
        logger.info("â¹ï¸ Ø¥ÙŠÙ‚Ø§Ù Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØªÙ…ØªØ©")
    
    def _run_scheduler(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„"""
        while self.is_running:
            self.scheduler.run_pending()
            time.sleep(1)
    
    def run_automation_cycle(self):
        """Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ØªÙ…ØªØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        start_time = datetime.now()
        logger.info("ðŸ”„ Ø¨Ø¯Ø¡ Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ØªÙ…ØªØ© Ø§Ù„Ø«ÙˆØ±ÙŠØ©")
        
        try:
            # 1. Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø±
            articles = self._scrape_all_sources()
            
            # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
            processed_articles = self._process_articles_with_ai(articles)
            
            # 3. Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            saved_count = self._save_articles_to_db(processed_articles)
            
            # 4. Ù†Ø´Ø± Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
            published_count = self._publish_articles(processed_articles)
            
            # 5. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_stats(len(articles), len(processed_articles), published_count, processing_time)
            
            logger.info(f"âœ… Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ØªÙ…ØªØ© Ù…ÙƒØªÙ…Ù„Ø©: {len(articles)} Ù…Ù‚Ø§Ù„ ØªÙ… Ø¬Ù…Ø¹Ù‡ØŒ {published_count} Ù…Ù‚Ø§Ù„ ØªÙ… Ù†Ø´Ø±Ù‡ ÙÙŠ {processing_time:.2f} Ø«Ø§Ù†ÙŠØ©")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ØªÙ…ØªØ©: {str(e)}")
            self.processing_stats['errors'] += 1
    
    def _scrape_all_sources(self) -> List[Dict]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
        all_articles = []
        
        for source in self.sources:
            if not source['active']:
                continue
                
            try:
                articles = self._scrape_source(source)
                all_articles.extend(articles)
                logger.info(f"ðŸ“¡ ØªÙ… Ø¬Ù…Ø¹ {len(articles)} Ù…Ù‚Ø§Ù„ Ù…Ù† {source['name']}")
                
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† {source['name']}: {str(e)}")
        
        return all_articles
    
    def _scrape_source(self, source: Dict) -> List[Dict]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ØµØ¯Ø± ÙˆØ§Ø­Ø¯"""
        articles = []
        
        try:
            # Ø¬Ù„Ø¨ RSS feed
            feed = feedparser.parse(source['url'])
            
            for entry in feed.entries[:5]:  # Ø¢Ø®Ø± 5 Ù…Ù‚Ø§Ù„Ø§Øª
                article = {
                    'title': entry.get('title', ''),
                    'content': entry.get('summary', ''),
                    'url': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'source': source['name'],
                    'country': source['country'],
                    'language': source['language']
                }
                
                # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
                if article['url']:
                    try:
                        full_content = self._scrape_full_content(article['url'])
                        if full_content:
                            article['content'] = full_content
                    except:
                        pass  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù„Ø®Øµ Ø¥Ø°Ø§ ÙØ´Ù„ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„
                
                articles.append(article)
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ RSS Ù…Ù† {source['name']}: {str(e)}")
        
        return articles
    
    def _scrape_full_content(self, url: str) -> str:
        """Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† URL"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            content_selectors = [
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                'main'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
                    for elem in content_elem.find_all(['script', 'style', 'nav', 'header', 'footer']):
                        elem.decompose()
                    
                    text = content_elem.get_text(strip=True)
                    if len(text) > 100:  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø­ØªÙˆÙ‰ ÙƒØ§ÙÙ
                        return text
            
            return ""
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† {url}: {str(e)}")
            return ""
    
    def _process_articles_with_ai(self, articles: List[Dict]) -> List[Dict]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        processed_articles = []
        
        for article in articles:
            try:
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
                ai_result = ai_engine.process_article_revolutionary(
                    content=article['content'],
                    title=article['title']
                )
                
                # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                processed_article = {
                    **article,
                    'ai_processed': True,
                    'summary': ai_result.get('summary', article.get('content', '')[:200]),
                    'category': ai_result.get('category', 'GENERAL'),
                    'sentiment_score': ai_result.get('sentiment_score', 0.5),
                    'keywords': ai_result.get('keywords', []),
                    'improved_title': ai_result.get('improved_title', article['title']),
                    'bias_analysis': ai_result.get('bias_analysis', {}),
                    'ai_confidence': ai_result.get('ai_confidence', 0.5),
                    'processing_time': ai_result.get('processing_time', 0)
                }
                
                processed_articles.append(processed_article)
                
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‚Ø§Ù„: {str(e)}")
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¯ÙˆÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© AI
                article['ai_processed'] = False
                processed_articles.append(article)
        
        return processed_articles
    
    def _save_articles_to_db(self, articles: List[Dict]) -> int:
        """Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        saved_count = 0
        
        try:
            conn = sqlite3.connect('golan24_revolutionary.db')
            cursor = conn.cursor()
            
            for article in articles:
                try:
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…Ø³Ø¨Ù‚Ø§Ù‹
                    cursor.execute('''
                        SELECT id FROM articles 
                        WHERE title = ? AND source = ?
                    ''', (article['title'], article['source']))
                    
                    if cursor.fetchone():
                        continue  # Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ø§Ù‹
                    
                    # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
                    cursor.execute('''
                        INSERT INTO articles (
                            title, content, summary, category, status,
                            published_at, views, ai_processed, sentiment_score,
                            keywords, source
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        article['improved_title'],
                        article['content'],
                        article['summary'],
                        article['category'],
                        'published',
                        datetime.now().isoformat(),
                        0,
                        article['ai_processed'],
                        article['sentiment_score'],
                        json.dumps(article['keywords']),
                        article['source']
                    ))
                    
                    saved_count += 1
                    
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§Ù„: {str(e)}")
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
        
        return saved_count
    
    def _publish_articles(self, articles: List[Dict]) -> int:
        """Ù†Ø´Ø± Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª"""
        published_count = 0
        
        for article in articles:
            try:
                # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ø·Ù‚ Ø§Ù„Ù†Ø´Ø± Ø§Ù„ÙØ¹Ù„ÙŠ
                # Ù…Ø«Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ
                # Ø£Ùˆ Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø±Ø§Øª Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
                
                logger.info(f"ðŸ“° Ù†Ø´Ø± Ù…Ù‚Ø§Ù„: {article['improved_title']}")
                published_count += 1
                
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù†Ø´Ø± Ø§Ù„Ù…Ù‚Ø§Ù„: {str(e)}")
        
        return published_count
    
    def _update_stats(self, scraped: int, processed: int, published: int, processing_time: float):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        self.processing_stats['total_scraped'] += scraped
        self.processing_stats['total_processed'] += processed
        self.processing_stats['total_published'] += published
        self.processing_stats['processing_time'] = processing_time
        self.processing_stats['last_run'] = datetime.now().isoformat()
    
    def run_analytics_update(self):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        logger.info("ðŸ“Š ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
        
        try:
            conn = sqlite3.connect('golan24_revolutionary.db')
            cursor = conn.cursor()
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
            cursor.execute('SELECT COUNT(*) FROM articles')
            total_articles = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(*) FROM articles WHERE ai_processed = 1')
            ai_processed = cursor.fetchone()[0]
            
            cursor.execute('SELECT AVG(sentiment_score) FROM articles WHERE sentiment_score IS NOT NULL')
            avg_sentiment = cursor.fetchone()[0] or 0
            
            # Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            analytics_data = {
                'total_articles': total_articles,
                'ai_processed_articles': ai_processed,
                'average_sentiment': avg_sentiment,
                'automation_runs': self.processing_stats['total_scraped'],
                'timestamp': datetime.now().isoformat()
            }
            
            # ÙŠÙ…ÙƒÙ† Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙÙŠ Ø¬Ø¯ÙˆÙ„ Ù…Ù†ÙØµÙ„
            cursor.execute('''
                INSERT INTO analytics (metric_name, metric_value, timestamp)
                VALUES (?, ?, ?)
            ''', ('daily_stats', json.dumps(analytics_data), datetime.now().isoformat()))
            
            conn.commit()
            conn.close()
            
            logger.info(f"ðŸ“ˆ ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {total_articles} Ù…Ù‚Ø§Ù„ØŒ {ai_processed} Ù…Ø¹Ø§Ù„Ø¬ Ø¨Ø§Ù„AI")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {str(e)}")
    
    def run_daily_maintenance(self):
        """Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©"""
        logger.info("ðŸ”§ Ø¨Ø¯Ø¡ Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©")
        
        try:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            # ÙØ­Øµ ØµØ­Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            # Ø¥Ø±Ø³Ø§Ù„ ØªÙ‚Ø±ÙŠØ± ÙŠÙˆÙ…ÙŠ
            
            logger.info("âœ… Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù…ÙƒØªÙ…Ù„Ø©")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©: {str(e)}")
    
    def get_automation_status(self) -> Dict:
        """Ø­Ø§Ù„Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØªÙ…ØªØ©"""
        return {
            'is_running': self.is_running,
            'processing_stats': self.processing_stats,
            'active_sources': len([s for s in self.sources if s['active']]),
            'total_sources': len(self.sources),
            'next_run': '5 minutes' if self.is_running else 'stopped',
            'automation_level': '100%',
            'processing_speed': f"{self.processing_stats.get('processing_time', 0):.2f} seconds"
        }

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø¹Ø§Ù„Ù…ÙŠ
automation_system = RevolutionaryAutomationSystem()
